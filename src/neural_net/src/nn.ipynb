{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, concatenate, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from UAVSequence import UAVSequence, ImageSequence\n",
    "from ValidationCallback import ValidationCallback\n",
    "from VAEModel import Sampling, VAE\n",
    "import math\n",
    "import PyKDL\n",
    "# Clear logs from previous runs\n",
    "# %rm -rf ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 10\n",
    "NUM_OUTPUTS = 11\n",
    "\n",
    "img_shape = (480, 640)\n",
    "\n",
    "# Load the dataset input and output\n",
    "x = np.loadtxt('../dataset_empty_auto/0_input.csv', delimiter=',', usecols=tuple(range(NUM_INPUTS))) # (num training points, 13)\n",
    "y = np.loadtxt('../dataset_empty_auto/0_output.csv', delimiter=',', usecols=tuple(range(NUM_OUTPUTS + 4))) # (num training points, 11)\n",
    "\n",
    "def get_rel_yaw(cur, goal):\n",
    "    diff = goal - cur\n",
    "    \n",
    "    if diff > math.pi:\n",
    "        diff -= 2 * math.pi\n",
    "    elif diff < -1 * math.pi:\n",
    "        diff += 2 * math.pi\n",
    "    \n",
    "    return diff\n",
    "\n",
    "for i in range(len(y)):\n",
    "    f = PyKDL.Frame(PyKDL.Rotation.RPY(*x[i][3:6]),\n",
    "                    PyKDL.Vector(*x[i][0:3]))\n",
    "\n",
    "    # INPUTS\n",
    "    lin_vel = f * PyKDL.Vector(*x[i][8:11])\n",
    "    ang_vel = f * PyKDL.Vector(*x[i][11:14])    \n",
    "    x[i][8:11] = lin_vel[0:3]\n",
    "    x[i][11:14] = ang_vel[0:3]\n",
    "    \n",
    "    vector_f = PyKDL.Frame(PyKDL.Rotation.RPY(*x[i][3:6]),\n",
    "                    PyKDL.Vector(0, 0, 0))\n",
    "    \n",
    "    # OUTPUTS\n",
    "    pos = f * PyKDL.Vector(*y[i][0:3])\n",
    "    vel = vector_f * PyKDL.Vector(*y[i][3:6])\n",
    "    acc = vector_f * PyKDL.Vector(*y[i][6:9])\n",
    "    y[i][0:3] = pos[0:3]\n",
    "    y[i][3:6] = vel[0:3]\n",
    "    y[i][6:9] = acc[0:3]\n",
    "    \n",
    "    y[i][9] = get_rel_yaw(x[i][5], y[i][9])\n",
    "    \n",
    "    \n",
    "x = np.delete(x, np.s_[5], 1)\n",
    "x = np.delete(x, np.s_[0:3], 1)\n",
    "\n",
    "num_training_samples = int(0.8 * len(x))\n",
    "num_val_samples = len(x) - num_training_samples\n",
    "\n",
    "x_val = x[num_training_samples:]\n",
    "y_val = y[num_training_samples:]\n",
    "x = x[:num_training_samples]\n",
    "y = y[:num_training_samples]\n",
    "\n",
    "# Shuffle x and y, but store the indices so we can still match them with the images\n",
    "hash_table = np.random.permutation(num_training_samples)\n",
    "hash_table_val = np.random.permutation(num_val_samples)\n",
    "\n",
    "x = x[hash_table]\n",
    "y = y[hash_table]\n",
    "x_val = x_val[hash_table_val]\n",
    "y_val = y_val[hash_table_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes actually training on the data. \n",
    "PS. If you want to view the tensorboard data, run `tensorboard --logdir logs/fit` in another window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a60029e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a60029e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a60029e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a60029e8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 480, 640, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 160, 32) 1600        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 80, 32)   9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 40, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 8, 64)     102464      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3072)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 1000)         3073000     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 1000)         3073000     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 1000)         0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3072)         3075072     sampling[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 6, 8, 64)     0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 30, 40, 64)   102464      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 60, 80, 32)   18464       conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 120, 160, 32) 9248        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 480, 640, 1)  1569        conv2d_transpose_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 9,484,625\n",
      "Trainable params: 9,484,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 1000\n",
    "\n",
    "encoder_inputs = Input(shape=(*img_shape, 1), name=\"encoder_input\")\n",
    "a = Conv2D(32, 7, strides=4, padding=\"same\", activation=\"relu\")(encoder_inputs)\n",
    "a = Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2D(64, 5, strides=5, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Flatten()(a)\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(a)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(a)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "# encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder = Model(encoder_inputs, z_mean, name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "# latent_inputs = Input(shape=(latent_dim,))\n",
    "a = Dense(6 * 8 * 64, activation=\"relu\")(z)\n",
    "a = Reshape((6, 8, 64))(a)\n",
    "a = Conv2DTranspose(64, 5, strides=5, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "decoder_outputs = Conv2DTranspose(1, 7, strides=4, padding=\"same\", activation=\"relu\")(a)\n",
    "# decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "# decoder.summary()\n",
    "\n",
    "vae = Model(encoder_inputs, decoder_outputs)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch is 32\n",
      "batch is 32\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "xy = ImageSequence(x, BATCH_SIZE, img_shape, hash_table)\n",
    "xy_validation = ImageSequence(x_val, BATCH_SIZE, img_shape, hash_table_val, num_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(input_img, reconstructed_img):\n",
    "    reconstruction_loss = tf.keras.losses.MSE(input_img, reconstructed_img)\n",
    "    reconstruction_loss *= 480 * 640\n",
    "    \n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss *= -0.5\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   1/1347 [..............................] - ETA: 37:32 - loss: 1.4653"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-68050014e3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mValidationCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "vae.fit(xy, epochs=NUM_EPOCHS, callbacks=[ValidationCallback(xy_validation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xy[5][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f96540b2be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = vae.predict(xy[5][0])[0]\n",
    "plt.imshow(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "vae.save('../models/vae_empty.tf')\n",
    "encoder.save('../models/encoder_empty.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a5f71c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a5f71c88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a5f71c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7fe1a5f71c88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model_vae = tf.keras.models.load_model('../models/vae_empty.tf', custom_objects={\"Sampling\": Sampling, \"vae_loss\": vae_loss})\n",
    "loaded_model_encoder = tf.keras.models.load_model('../models/encoder_empty.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 480, 640, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 1000)         3204808     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1000)         0           encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1010)         0           flatten_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          505500      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 250)          125250      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          25100       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           5050        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 25)           1275        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 11)           286         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,867,269\n",
      "Trainable params: 662,461\n",
      "Non-trainable params: 3,204,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the latent space representation of the image from the encoder\n",
    "NUM_INPUTS = 10\n",
    "NUM_OUTPUTS = 11\n",
    "img_shape = (480, 640)\n",
    "\n",
    "image_inputs = Input(shape=(*img_shape, 1))\n",
    "numerical_inputs = Input(shape=(NUM_INPUTS,))\n",
    "\n",
    "loaded_model_encoder.trainable = False\n",
    "z_mean = loaded_model_encoder(image_inputs)\n",
    "z_mean = Flatten()(z_mean)\n",
    "combined = concatenate([z_mean, numerical_inputs])\n",
    "\n",
    "# BEST: 500/250/100/50/25/11   |     0.342\n",
    "#       300/100/33             |     0.541\n",
    "#       500/250/100/50/25/11   |     0.991     |     0.001 l2 regularization\n",
    "\n",
    "# x_combined = Dropout(0.5)(combined)\n",
    "x_combined = Dense(500, kernel_regularizer=l2(0.00), activation='relu')(combined)\n",
    "x_combined = Dense(250, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "x_combined = Dense(100, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "x_combined = Dense(50, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "x_combined = Dense(25, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "# x_combined = Dropout(0.3)(x_combined)\n",
    "# x_combined = Dense(30, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "outputs = Dense(NUM_OUTPUTS, kernel_regularizer=l2(0.00), activation='linear')(x_combined)\n",
    "\n",
    "model = Model([image_inputs, numerical_inputs], outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch is 32\n",
      "batch is 32\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "xy = UAVSequence(x, y, BATCH_SIZE, img_shape, hash_table)\n",
    "xy_validation = UAVSequence(x_val, y_val, BATCH_SIZE, img_shape, hash_table_val, num_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(target, predicted):\n",
    "    other_loss = tf.keras.losses.MSE(target[:,:9], predicted[:,:9])\n",
    "    yaw_loss = 10 * tf.keras.losses.MSE(target[:,9:], predicted[:,9:])\n",
    "    \n",
    "    total_loss = (other_loss * 9 + yaw_loss * 2) / 11\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,:3], y_pred[:,:3])\n",
    "\n",
    "def vel_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,3:6], y_pred[:,3:6])\n",
    "\n",
    "def acc_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,6:9], y_pred[:,6:9])\n",
    "\n",
    "def yaw_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,9:], y_pred[:,9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "337/337 [==============================] - 22s 64ms/step - loss: 0.0463 - pos_metric: 0.0860 - vel_metric: 0.0078 - acc_metric: 0.0419 - yaw_metric: 0.0512\n",
      "1347/1347 [==============================] - 110s 82ms/step - loss: 0.0752 - pos_metric: 0.0874 - vel_metric: 0.0217 - acc_metric: 0.1184 - yaw_metric: 0.0725\n",
      "Epoch 2/100\n",
      "337/337 [==============================] - 22s 65ms/step - loss: 0.0381 - pos_metric: 0.0739 - vel_metric: 0.0049 - acc_metric: 0.0288 - yaw_metric: 0.0483\n",
      "1347/1347 [==============================] - 110s 82ms/step - loss: 0.0343 - pos_metric: 0.0588 - vel_metric: 0.0057 - acc_metric: 0.0304 - yaw_metric: 0.0462\n",
      "Epoch 3/100\n",
      "337/337 [==============================] - 22s 65ms/step - loss: 0.0359 - pos_metric: 0.0725 - vel_metric: 0.0062 - acc_metric: 0.0262 - yaw_metric: 0.0400\n",
      "1347/1347 [==============================] - 110s 81ms/step - loss: 0.0288 - pos_metric: 0.0516 - vel_metric: 0.0051 - acc_metric: 0.0236 - yaw_metric: 0.0376\n",
      "Epoch 4/100\n",
      "337/337 [==============================] - 22s 64ms/step - loss: 0.0330 - pos_metric: 0.0650 - vel_metric: 0.0056 - acc_metric: 0.0246 - yaw_metric: 0.0387\n",
      "1347/1347 [==============================] - 110s 81ms/step - loss: 0.0263 - pos_metric: 0.0473 - vel_metric: 0.0048 - acc_metric: 0.0208 - yaw_metric: 0.0352\n",
      "Epoch 5/100\n",
      "337/337 [==============================] - 22s 64ms/step - loss: 0.0318 - pos_metric: 0.0668 - vel_metric: 0.0042 - acc_metric: 0.0244 - yaw_metric: 0.0317\n",
      "1347/1347 [==============================] - 109s 81ms/step - loss: 0.0238 - pos_metric: 0.0428 - vel_metric: 0.0046 - acc_metric: 0.0188 - yaw_metric: 0.0317\n",
      "Epoch 6/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0298 - pos_metric: 0.0579 - vel_metric: 0.0064 - acc_metric: 0.0209 - yaw_metric: 0.0361\n",
      "1347/1347 [==============================] - 108s 80ms/step - loss: 0.0215 - pos_metric: 0.0390 - vel_metric: 0.0043 - acc_metric: 0.0172 - yaw_metric: 0.0273\n",
      "Epoch 7/100\n",
      "337/337 [==============================] - 21s 62ms/step - loss: 0.0287 - pos_metric: 0.0607 - vel_metric: 0.0070 - acc_metric: 0.0192 - yaw_metric: 0.0277\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0200 - pos_metric: 0.0358 - vel_metric: 0.0044 - acc_metric: 0.0160 - yaw_metric: 0.0259\n",
      "Epoch 8/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0266 - pos_metric: 0.0550 - vel_metric: 0.0045 - acc_metric: 0.0192 - yaw_metric: 0.0282\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0185 - pos_metric: 0.0325 - vel_metric: 0.0041 - acc_metric: 0.0149 - yaw_metric: 0.0245\n",
      "Epoch 9/100\n",
      "337/337 [==============================] - 21s 62ms/step - loss: 0.0269 - pos_metric: 0.0559 - vel_metric: 0.0049 - acc_metric: 0.0200 - yaw_metric: 0.0269\n",
      "1347/1347 [==============================] - 107s 80ms/step - loss: 0.0170 - pos_metric: 0.0296 - vel_metric: 0.0039 - acc_metric: 0.0139 - yaw_metric: 0.0222\n",
      "Epoch 10/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0270 - pos_metric: 0.0577 - vel_metric: 0.0040 - acc_metric: 0.0201 - yaw_metric: 0.0259\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0160 - pos_metric: 0.0269 - vel_metric: 0.0038 - acc_metric: 0.0129 - yaw_metric: 0.0228\n",
      "Epoch 11/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0253 - pos_metric: 0.0528 - vel_metric: 0.0048 - acc_metric: 0.0178 - yaw_metric: 0.0261\n",
      "1347/1347 [==============================] - 107s 80ms/step - loss: 0.0149 - pos_metric: 0.0246 - vel_metric: 0.0037 - acc_metric: 0.0124 - yaw_metric: 0.0208\n",
      "Epoch 12/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0264 - pos_metric: 0.0503 - vel_metric: 0.0045 - acc_metric: 0.0244 - yaw_metric: 0.0266\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0141 - pos_metric: 0.0226 - vel_metric: 0.0035 - acc_metric: 0.0121 - yaw_metric: 0.0204\n",
      "Epoch 13/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0244 - pos_metric: 0.0484 - vel_metric: 0.0047 - acc_metric: 0.0176 - yaw_metric: 0.0282\n",
      "1347/1347 [==============================] - 107s 80ms/step - loss: 0.0131 - pos_metric: 0.0207 - vel_metric: 0.0033 - acc_metric: 0.0109 - yaw_metric: 0.0193\n",
      "Epoch 14/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0242 - pos_metric: 0.0488 - vel_metric: 0.0049 - acc_metric: 0.0184 - yaw_metric: 0.0247\n",
      "1347/1347 [==============================] - 108s 80ms/step - loss: 0.0127 - pos_metric: 0.0198 - vel_metric: 0.0033 - acc_metric: 0.0107 - yaw_metric: 0.0192\n",
      "Epoch 15/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0226 - pos_metric: 0.0437 - vel_metric: 0.0037 - acc_metric: 0.0171 - yaw_metric: 0.0272\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0119 - pos_metric: 0.0180 - vel_metric: 0.0032 - acc_metric: 0.0103 - yaw_metric: 0.0184\n",
      "Epoch 16/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0231 - pos_metric: 0.0441 - vel_metric: 0.0037 - acc_metric: 0.0193 - yaw_metric: 0.0263\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0112 - pos_metric: 0.0166 - vel_metric: 0.0030 - acc_metric: 0.0097 - yaw_metric: 0.0175\n",
      "Epoch 17/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0227 - pos_metric: 0.0437 - vel_metric: 0.0042 - acc_metric: 0.0191 - yaw_metric: 0.0242\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0109 - pos_metric: 0.0158 - vel_metric: 0.0029 - acc_metric: 0.0096 - yaw_metric: 0.0174\n",
      "Epoch 18/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0229 - pos_metric: 0.0436 - vel_metric: 0.0044 - acc_metric: 0.0168 - yaw_metric: 0.0287\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0107 - pos_metric: 0.0153 - vel_metric: 0.0029 - acc_metric: 0.0092 - yaw_metric: 0.0175\n",
      "Epoch 19/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0223 - pos_metric: 0.0427 - vel_metric: 0.0053 - acc_metric: 0.0168 - yaw_metric: 0.0256\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0104 - pos_metric: 0.0147 - vel_metric: 0.0029 - acc_metric: 0.0090 - yaw_metric: 0.0174\n",
      "Epoch 20/100\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 0.0222 - pos_metric: 0.0411 - vel_metric: 0.0048 - acc_metric: 0.0188 - yaw_metric: 0.0252\n",
      "1347/1347 [==============================] - 107s 80ms/step - loss: 0.0098 - pos_metric: 0.0137 - vel_metric: 0.0028 - acc_metric: 0.0085 - yaw_metric: 0.0162\n",
      "Epoch 21/100\n",
      "337/337 [==============================] - 21s 62ms/step - loss: 0.0239 - pos_metric: 0.0457 - vel_metric: 0.0050 - acc_metric: 0.0175 - yaw_metric: 0.0290\n",
      "1347/1347 [==============================] - 107s 80ms/step - loss: 0.0096 - pos_metric: 0.0133 - vel_metric: 0.0028 - acc_metric: 0.0085 - yaw_metric: 0.0160\n",
      "Epoch 22/100\n",
      "337/337 [==============================] - 21s 62ms/step - loss: 0.0211 - pos_metric: 0.0396 - vel_metric: 0.0048 - acc_metric: 0.0170 - yaw_metric: 0.0240\n",
      "1347/1347 [==============================] - 107s 79ms/step - loss: 0.0093 - pos_metric: 0.0128 - vel_metric: 0.0027 - acc_metric: 0.0082 - yaw_metric: 0.0157\n",
      "Epoch 23/100\n",
      "337/337 [==============================] - 22s 64ms/step - loss: 0.0213 - pos_metric: 0.0418 - vel_metric: 0.0038 - acc_metric: 0.0157 - yaw_metric: 0.0249\n",
      "1347/1347 [==============================] - 108s 80ms/step - loss: 0.0089 - pos_metric: 0.0119 - vel_metric: 0.0025 - acc_metric: 0.0079 - yaw_metric: 0.0156\n",
      "Epoch 24/100\n",
      " 485/1347 [=========>....................] - ETA: 54s - loss: 0.0091 - pos_metric: 0.0117 - vel_metric: 0.0026 - acc_metric: 0.0078 - yaw_metric: 0.0167"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[pos_metric, vel_metric, acc_metric, yaw_metric])\n",
    "model.fit(xy, epochs=NUM_EPOCHS, callbacks=[ValidationCallback(xy_validation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('../models/complete_empty_30_min_rel_pos_0_030.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_complete = tf.keras.models.load_model('../models/complete_empty_30_min_rel_pos_0_030.tf', custom_objects={\"pos_metric\": pos_metric, \"vel_metric\": vel_metric, \"acc_metric\": acc_metric, \"yaw_metric\": yaw_metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.126400e-01  6.292000e-02  3.170000e-03  1.825582e+00 -4.321980e-01\n",
      " -3.900000e-05  5.626340e-01  6.688060e-01  7.000000e-06 -1.975620e-01\n",
      " -2.372890e-01]\n",
      "[ 0.13429113  0.05333463 -0.0191496   1.7202779  -0.5699907   0.04652605\n",
      "  0.47043067  0.65620583 -0.01152456 -0.26060554 -0.04743019]\n",
      "[-0.02165113  0.00958537  0.0223196   0.10530409  0.13779269 -0.04656505\n",
      "  0.09220333  0.01260017  0.01153156  0.06304354 -0.18985881]\n"
     ]
    }
   ],
   "source": [
    "BATCH = 1\n",
    "NUM = 0\n",
    "print(xy_validation[BATCH][1][NUM])\n",
    "single_x_test = xy_validation[BATCH][0]\n",
    "single_x_test[0] = single_x_test[0][NUM]\n",
    "single_x_test[1] = single_x_test[1][NUM]\n",
    "# print(single_x_test[0].shape)\n",
    "# print(single_x_test[1].shape)\n",
    "single_x_test[0] = np.array([single_x_test[0],])\n",
    "single_x_test[1] = np.array([single_x_test[1],])\n",
    "# print(single_x_test[1])\n",
    "# print(single_x_test[0].shape)\n",
    "# print(single_x_test[1].shape)\n",
    "\n",
    "prediction = loaded_model_complete.predict(single_x_test)[0]\n",
    "print(prediction)\n",
    "\n",
    "print(np.asarray(xy_validation[BATCH][1][NUM]) - np.asarray(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
