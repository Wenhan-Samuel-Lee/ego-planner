{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, concatenate, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from UAVSequence import UAVSequence, ImageSequence\n",
    "from ValidationCallback import ValidationCallback\n",
    "from VAEModel import Sampling, VAE\n",
    "import math\n",
    "import transforms3d\n",
    "# Clear logs from previous runs\n",
    "# %rm -rf ../logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_INPUTS = 10\n",
    "NUM_OUTPUTS = 11\n",
    "\n",
    "img_shape = (480, 640)\n",
    "\n",
    "# Load the dataset input and output\n",
    "x = np.loadtxt('../dataset_empty_30_transformed_12_10/0_input.csv', delimiter=',', usecols=tuple(range(NUM_INPUTS + 4))) # (num training points, 13)\n",
    "y = np.loadtxt('../dataset_empty_30_transformed_12_10/0_output.csv', delimiter=',', usecols=tuple(range(NUM_OUTPUTS))) # (num training points, 11)\n",
    "\n",
    "def get_rel_yaw(cur, goal):\n",
    "    diff = goal - cur\n",
    "    \n",
    "    if diff > math.pi:\n",
    "        diff -= 2 * math.pi\n",
    "    elif diff < -1 * math.pi:\n",
    "        diff += 2 * math.pi\n",
    "    \n",
    "    return diff\n",
    "\n",
    "for i in range(len(y)):\n",
    "    translation = x[i][0:3]\n",
    "    rot_matrix = transforms3d.euler.euler2mat(*x[i][3:6])\n",
    "    \n",
    "    # INPUTS\n",
    "    # linear velocities\n",
    "    x[i][8:11] = np.matmul(rot_matrix, x[i][8:11])\n",
    "    # angular velocities\n",
    "    x[i][11:14] = np.matmul(rot_matrix, x[i][11:14])\n",
    "    \n",
    "    # OUTPUTS\n",
    "    # relative positions\n",
    "    y[i][0:3] = np.matmul(rot_matrix, np.subtract(y[i][0:3], translation))\n",
    "    # relative velocities\n",
    "    y[i][3:6] = np.matmul(rot_matrix, y[i][3:6])\n",
    "    # relative accelerations\n",
    "    y[i][6:9] = np.matmul(rot_matrix, y[i][6:9])\n",
    "    # relative yaw\n",
    "    y[i][9] = get_rel_yaw(x[i][5], y[i][9])\n",
    "    \n",
    "    \n",
    "x = np.delete(x, np.s_[5], 1)\n",
    "x = np.delete(x, np.s_[0:3], 1)\n",
    "\n",
    "num_training_samples = int(0.8 * len(x))\n",
    "num_val_samples = len(x) - num_training_samples\n",
    "\n",
    "x_val = x[num_training_samples:]\n",
    "y_val = y[num_training_samples:]\n",
    "x = x[:num_training_samples]\n",
    "y = y[:num_training_samples]\n",
    "\n",
    "# Shuffle x and y, but store the indices so we can still match them with the images\n",
    "hash_table = np.random.permutation(num_training_samples)\n",
    "hash_table_val = np.random.permutation(num_val_samples)\n",
    "\n",
    "x = x[hash_table]\n",
    "y = y[hash_table]\n",
    "x_val = x_val[hash_table_val]\n",
    "y_val = y_val[hash_table_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation = [1, 1, 1]\n",
    "# rot_matrix = transforms3d.euler.euler2mat(0, 0, math.pi / 2)\n",
    "\n",
    "# print(np.matmul(rot_matrix, [1, 0, 0]))\n",
    "# print(np.matmul(rot_matrix, np.subtract([1, 2, 1], translation)))\n",
    "\n",
    "# INPUTS\n",
    "# x[i][8:11] = np.matmul(rot_matrix, x[i][8:11])\n",
    "# x[i][11:14] = np.matmul(rot_matrix, x[i][11:14])\n",
    "\n",
    "# # OUTPUTS\n",
    "# y[i][0:3] = np.matmul(drone_frame, y[i][0:3])\n",
    "# y[i][3:6] = np.matmul(rot_matrix, y[i][3:6])\n",
    "# y[i][6:9] = np.matmul(rot_matrix, y[i][6:9])\n",
    "\n",
    "# y[i][9] = get_rel_yaw(x[i][5], y[i][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes actually training on the data. \n",
    "PS. If you want to view the tensorboard data, run `tensorboard --logdir logs/fit` in another window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34db358390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34db358390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34db358390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34db358390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 480, 640, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 160, 32) 1600        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 80, 32)   9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 40, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 8, 64)     102464      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3072)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 1000)         3073000     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 1000)         3073000     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 1000)         0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3072)         3075072     sampling[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 6, 8, 64)     0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 30, 40, 64)   102464      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 60, 80, 32)   18464       conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 120, 160, 32) 9248        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 480, 640, 1)  1569        conv2d_transpose_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 9,484,625\n",
      "Trainable params: 9,484,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 1000\n",
    "\n",
    "encoder_inputs = Input(shape=(*img_shape, 1), name=\"encoder_input\")\n",
    "a = Conv2D(32, 7, strides=4, padding=\"same\", activation=\"relu\")(encoder_inputs)\n",
    "a = Conv2D(32, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2D(64, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2D(64, 5, strides=5, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Flatten()(a)\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(a)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(a)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "# encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder = Model(encoder_inputs, z_mean, name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "# latent_inputs = Input(shape=(latent_dim,))\n",
    "a = Dense(6 * 8 * 64, activation=\"relu\")(z)\n",
    "a = Reshape((6, 8, 64))(a)\n",
    "a = Conv2DTranspose(64, 5, strides=5, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "a = Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(a)\n",
    "decoder_outputs = Conv2DTranspose(1, 7, strides=4, padding=\"same\", activation=\"relu\")(a)\n",
    "# decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "# decoder.summary()\n",
    "\n",
    "vae = Model(encoder_inputs, decoder_outputs)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch is 32\n",
      "batch is 32\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "xy = ImageSequence(x, BATCH_SIZE, img_shape, hash_table)\n",
    "xy_validation = ImageSequence(x_val, BATCH_SIZE, img_shape, hash_table_val, num_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(input_img, reconstructed_img):\n",
    "    reconstruction_loss = tf.keras.losses.MSE(input_img, reconstructed_img)\n",
    "    reconstruction_loss *= 480 * 640\n",
    "    \n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss *= -0.5\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   1/1347 [..............................] - ETA: 37:32 - loss: 1.4653"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-68050014e3de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mValidationCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "vae.fit(xy, epochs=NUM_EPOCHS, callbacks=[ValidationCallback(xy_validation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xy[5][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f96540b2be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = vae.predict(xy[5][0])[0]\n",
    "plt.imshow(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "vae.save('../models/vae_empty.tf')\n",
    "encoder.save('../models/encoder_empty.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/fri/UAV/ego-planner/src/neural_net/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34dc2a8518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34dc2a8518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34dc2a8518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Sampling.call of <VAEModel.Sampling object at 0x7f34dc2a8518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "loaded_model_vae = tf.keras.models.load_model('../models/vae_empty.tf', custom_objects={\"Sampling\": Sampling, \"vae_loss\": vae_loss})\n",
    "loaded_model_encoder = tf.keras.models.load_model('../models/encoder_empty.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 480, 640, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 1000)         3204808     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1000)         0           encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1010)         0           flatten_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          505500      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 250)          125250      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          25100       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           5050        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 25)           1275        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 11)           286         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,867,269\n",
      "Trainable params: 662,461\n",
      "Non-trainable params: 3,204,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the latent space representation of the image from the encoder\n",
    "NUM_INPUTS = 10\n",
    "NUM_OUTPUTS = 11\n",
    "img_shape = (480, 640)\n",
    "\n",
    "image_inputs = Input(shape=(*img_shape, 1))\n",
    "numerical_inputs = Input(shape=(NUM_INPUTS,))\n",
    "\n",
    "loaded_model_encoder.trainable = False\n",
    "z_mean = loaded_model_encoder(image_inputs)\n",
    "z_mean = Flatten()(z_mean)\n",
    "combined = concatenate([z_mean, numerical_inputs])\n",
    "\n",
    "# BEST: 500/250/100/50/25/11   |     0.342\n",
    "#       300/100/33             |     0.541\n",
    "#       500/250/100/50/25/11   |     0.991     |     0.001 l2 regularization\n",
    "\n",
    "# x_combined = Dropout(0.5)(combined)\n",
    "x_combined = Dense(500, kernel_regularizer=l2(0.00), activation='relu')(combined)\n",
    "x_combined = Dense(250, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "x_combined = Dense(100, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "x_combined = Dense(50, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "x_combined = Dense(25, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "# x_combined = Dropout(0.3)(x_combined)\n",
    "# x_combined = Dense(30, kernel_regularizer=l2(0.00), activation='relu')(x_combined)\n",
    "outputs = Dense(NUM_OUTPUTS, kernel_regularizer=l2(0.00), activation='linear')(x_combined)\n",
    "\n",
    "model = Model([image_inputs, numerical_inputs], outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch is 32\n",
      "batch is 32\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "xy = UAVSequence(x, y, BATCH_SIZE, img_shape, hash_table)\n",
    "xy_validation = UAVSequence(x_val, y_val, BATCH_SIZE, img_shape, hash_table_val, num_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(target, predicted):\n",
    "    other_loss = tf.keras.losses.MSE(target[:,:9], predicted[:,:9])\n",
    "    yaw_loss = 10 * tf.keras.losses.MSE(target[:,9:], predicted[:,9:])\n",
    "    \n",
    "    total_loss = (other_loss * 9 + yaw_loss * 2) / 11\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,:3], y_pred[:,:3])\n",
    "\n",
    "def vel_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,3:6], y_pred[:,3:6])\n",
    "\n",
    "def acc_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,6:9], y_pred[:,6:9])\n",
    "\n",
    "def yaw_metric(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true[:,9:], y_pred[:,9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "291/291 [==============================] - 19s 64ms/step - loss: 0.0318 - pos_metric: 9.1174e-04 - vel_metric: 0.0046 - acc_metric: 0.0905 - yaw_metric: 0.0311\n",
      "1164/1164 [==============================] - 97s 83ms/step - loss: 0.0077 - pos_metric: 7.8282e-04 - vel_metric: 0.0031 - acc_metric: 0.0126 - yaw_metric: 0.0175\n",
      "Epoch 2/100\n",
      " 710/1164 [=================>............] - ETA: 29s - loss: 0.0075 - pos_metric: 7.8107e-04 - vel_metric: 0.0030 - acc_metric: 0.0129 - yaw_metric: 0.0161"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.MSE, metrics=[pos_metric, vel_metric, acc_metric, yaw_metric])\n",
    "model.fit(xy, epochs=NUM_EPOCHS, callbacks=[ValidationCallback(xy_validation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('../models/complete_empty_30_min_transformed_0_034.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/complete_empty_30_min_transformed_0_034.tf', custom_objects={\"pos_metric\": pos_metric, \"vel_metric\": vel_metric, \"acc_metric\": acc_metric, \"yaw_metric\": yaw_metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.126400e-01  6.292000e-02  3.170000e-03  1.825582e+00 -4.321980e-01\n",
      " -3.900000e-05  5.626340e-01  6.688060e-01  7.000000e-06 -1.975620e-01\n",
      " -2.372890e-01]\n",
      "[ 0.13429113  0.05333463 -0.0191496   1.7202779  -0.5699907   0.04652605\n",
      "  0.47043067  0.65620583 -0.01152456 -0.26060554 -0.04743019]\n",
      "[-0.02165113  0.00958537  0.0223196   0.10530409  0.13779269 -0.04656505\n",
      "  0.09220333  0.01260017  0.01153156  0.06304354 -0.18985881]\n"
     ]
    }
   ],
   "source": [
    "BATCH = 1\n",
    "NUM = 0\n",
    "print(xy_validation[BATCH][1][NUM])\n",
    "single_x_test = xy_validation[BATCH][0]\n",
    "single_x_test[0] = single_x_test[0][NUM]\n",
    "single_x_test[1] = single_x_test[1][NUM]\n",
    "# print(single_x_test[0].shape)\n",
    "# print(single_x_test[1].shape)\n",
    "single_x_test[0] = np.array([single_x_test[0],])\n",
    "single_x_test[1] = np.array([single_x_test[1],])\n",
    "# print(single_x_test[1])\n",
    "# print(single_x_test[0].shape)\n",
    "# print(single_x_test[1].shape)\n",
    "\n",
    "prediction = loaded_model_complete.predict(single_x_test)[0]\n",
    "print(prediction)\n",
    "\n",
    "print(np.asarray(xy_validation[BATCH][1][NUM]) - np.asarray(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
